{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "659d83f1",
   "metadata": {},
   "source": [
    "# MNIST Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b0b73e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Vector{Int64}:\n",
       " 784\n",
       "  30\n",
       "  10"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using DelimitedFiles\n",
    "using StatsBase\n",
    "using Distributions\n",
    "using LinearAlgebra\n",
    "\n",
    "# read MNIST data\n",
    "const testx = readdlm(\"testx.csv\", ',', Int, '\\n')\n",
    "const testy = readdlm(\"testy.csv\", ',', Int, '\\n')\n",
    "const trainx = readdlm(\"trainx.csv\", ',', Int, '\\n')\n",
    "const trainy = readdlm(\"trainy.csv\", ',', Int, '\\n')\n",
    "\n",
    "const L = 3                 # number of layers including input and output\n",
    "const sizes = [784, 30, 10] # number of neurons in each layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31044618",
   "metadata": {},
   "source": [
    "The next section contains some helper functions to abstract the necessary tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a5ba91d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backprop (generic function with 1 method)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the activation function that will be used.\n",
    "@. sigmoid(x) = 1/(1 + exp(-x))      # sigmoid activation\n",
    "@. sigmoidPrime(x) = sigmoid(x) * (1-sigmoid(x))\n",
    "\n",
    "# HELPER: convert a digit d to a 10-element vector\n",
    "# e.g. 6 is converted to [0,0,0,0,0,0,1,0,0,0]\n",
    "function digit2vector(d)\n",
    "    vcat( repeat([0], d), 1, repeat([0], 9-d) )\n",
    "end\n",
    "\n",
    "# feedforward:\n",
    "# inputs:\n",
    "#    -W: matrix of weights in the NN\n",
    "#    -b: biases of the NN \n",
    "#    -x: input of a single training example (a vector of length 784)\n",
    "# returns the activations \n",
    "function feedforward(W, b, x)\n",
    "    # note that z[1] is not used. we put it there so that the indices make sense.\n",
    "    z = [ x, zeros(sizes[2]), zeros(sizes[3]) ]\n",
    "    a = [ x, zeros(sizes[2]), zeros(sizes[3]) ]\n",
    "    for l = 2:L\n",
    "        z[l] = W[l-1]*a[l-1] + b[l-1]\n",
    "        a[l] = sigmoid(z[l])\n",
    "    end\n",
    "    return a, z\n",
    "end\n",
    "\n",
    "# given an input vector, return the predicted digit\n",
    "function classify(W, b, x)\n",
    "    (a, z) = feedforward(W, b, x)\n",
    "    yhat = a[3]\n",
    "    convert(Int, findmax(yhat)[2] - 1)\n",
    "end\n",
    "\n",
    "# HELPER: ( backprop()).\n",
    "# this function computes the error for a single training example.\n",
    "# W contains the weights in the network.\n",
    "# a contains the activations.\n",
    "# z contains the weighted inputs.\n",
    "# y is the correct digit.\n",
    "# returns δ = the error. the size of δ is [ 784, 30, 10 ]\n",
    "function compute_error(W, a, z, y)\n",
    "    δ = [ zeros(sizes[1]), zeros(sizes[2]), zeros(sizes[3]) ]\n",
    "    # note that δ[1] is junk. we put it there so that the indices make sense.\n",
    "\n",
    "    # at the output layer L\n",
    "    δ[3] = -(digit2vector(y) .- a[3]) .* sigmoidPrime(z[3])\n",
    "\n",
    "    # for each earlier layer L-1,L-2,..,2 (for the HW, this means only layer 2)\n",
    "    δ[2] = W[2]' * δ[3] .* sigmoidPrime(z[2])\n",
    "\n",
    "    return δ\n",
    "end\n",
    "\n",
    "# helper function for backprop(). given the errors δ and the\n",
    "# activations a for a single training example, this function returns\n",
    "# the gradient components ∇W and ∇b.\n",
    "# this function implements teh equations BP3 and BP4.\n",
    "function compute_gradients(δ, a)\n",
    "    ∇W = [ zeros(sizes[2], sizes[1]),\n",
    "           zeros(sizes[3], sizes[2]) ]\n",
    "    ∇b = [ zeros(sizes[2]), zeros(sizes[3]) ]\n",
    "    ∇W[1] = δ[2] * a[1]'  # BP4\n",
    "    ∇b[1] = δ[2]          # BP3\n",
    "    ∇W[2] = δ[3] * a[2]'\n",
    "    ∇b[2] = δ[3]\n",
    "    return ∇W, ∇b\n",
    "end\n",
    "\n",
    "# backpropagation. returns ∇W and ∇b for a single training example.\n",
    "function backprop(W, b, x, y)\n",
    "    (a, z) = feedforward(W, b, x)\n",
    "    δ = compute_error(W, a, z, y)\n",
    "    (∇W, ∇b) = compute_gradients(δ, a)\n",
    "    return ∇W, ∇b\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e68f62a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BGD (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gradient descent algorithm.\n",
    "# W = weights in the network\n",
    "# b = biases in the network\n",
    "# batch = the indices of the observations in the batch, i.e. the rows of trainx\n",
    "# α = step size\n",
    "# λ = regularization parameter\n",
    "function GD(W, b, batch; α=0.01, λ=0.01)\n",
    "    m = length(batch)    # batch size\n",
    "\n",
    "    # data structure to accumulate the sum over the batch\n",
    "    # in the notes and in Ng's article sumW is ΔW and sumb is Δb.\n",
    "    sumW = [ zeros(sizes[2], sizes[1]),\n",
    "             zeros(sizes[3], sizes[2]) ]\n",
    "    sumb = [ zeros(sizes[2]), zeros(sizes[3]) ]\n",
    "\n",
    "    # for each training example in the batch, use backprop\n",
    "    # to compute the gradients and add them to the sum\n",
    "    for i in batch\n",
    "        x = trainx[i,:]\n",
    "        y = trainy[i]\n",
    "        (∇W, ∇b) = backprop(W, b, x, y)\n",
    "        sumW += ∇W\n",
    "        sumb += ∇b\n",
    "    end\n",
    "\n",
    "    # make the update to the weights and biases and take a step\n",
    "    # of gradient descent. note that we use the average gradient.\n",
    "    ∇W = (1/m)*sumW .+ λ*W\n",
    "    ∇b = (1/m)*sumb\n",
    "    W = W .- α * ∇W\n",
    "    b = b .- α * ∇b\n",
    "\n",
    "    # return the updated weights and biases. we also return the gradients\n",
    "    return W, b, ∇W, ∇b\n",
    "end\n",
    "\n",
    "# classify the test data and compute the classification accuracy\n",
    "function accuracy(W, b) \n",
    "    ntest = length(testy)\n",
    "    yhat = zeros(Int, ntest)\n",
    "    for i in 1:ntest\n",
    "        yhat[i] = classify(W, b, testx[i,:])\n",
    "    end\n",
    "    sum(testy .== yhat)/ntest # hit rate\n",
    "end\n",
    "\n",
    "# train the neural network using batch gradient descent.\n",
    "# this is a driver function to repeatedly call GD().\n",
    "# N = number of observations in the training data.\n",
    "# m = batch size\n",
    "# α = learning rate / step size\n",
    "# λ = regularization parameter\n",
    "function BGD(N, m, epochs; α=0.01, λ=0.01) \n",
    "    # random initialization of the weights and biases\n",
    "    d = Normal(0, 1)\n",
    "    W = [ rand(d, sizes[2], sizes[1]),  # layer 1 to 2\n",
    "          rand(d, sizes[3], sizes[2]) ] # layer 2 to 3\n",
    "    b = [ rand(d, sizes[2]),   # layer 2\n",
    "          rand(d, sizes[3]) ]  # layer 3\n",
    "    ∇W = [ zeros(sizes[2], sizes[1]),  # layer 1 to 2\n",
    "          zeros(sizes[3], sizes[2]) ] # layer 2 to 3\n",
    "    ∇b = [ zeros(sizes[2]),   # layer 2\n",
    "          zeros(sizes[3]) ]   # layer 3\n",
    "    for j in 1:epochs\n",
    "        remaining = 1:N\n",
    "        while length(remaining) > 0\n",
    "            batch = sample(remaining, m, replace=false)\n",
    "            remaining = setdiff(remaining, batch)\n",
    "            (W, b, ∇W, ∇b) = GD(W, b, batch; α=α, λ=λ)   \n",
    "        end\n",
    "        println(\"epoch \", j, \", accuracy = \", accuracy(W,b))\n",
    "    end\n",
    "    return W, b, ∇W, ∇b\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07961d36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, accuracy = 0.2051\n",
      "epoch 2, accuracy = 0.2815\n",
      "epoch 3, accuracy = 0.3488\n",
      "epoch 4, accuracy = 0.4218\n",
      "epoch 5, accuracy = 0.4912\n",
      "epoch 6, accuracy = 0.546\n",
      "epoch 7, accuracy = 0.6325\n",
      "epoch 8, accuracy = 0.7428\n",
      "epoch 9, accuracy = 0.8069\n",
      "epoch 10, accuracy = 0.848\n",
      "epoch 11, accuracy = 0.8632\n",
      "epoch 12, accuracy = 0.8776\n",
      "epoch 13, accuracy = 0.888\n",
      "epoch 14, accuracy = 0.8922\n",
      "epoch 15, accuracy = 0.898\n",
      "epoch 16, accuracy = 0.8929\n",
      "epoch 17, accuracy = 0.8922\n",
      "epoch 18, accuracy = 0.8952\n",
      "epoch 19, accuracy = 0.8986\n",
      "epoch 20, accuracy = 0.8952\n",
      "epoch 21, accuracy = 0.8988\n",
      "epoch 22, accuracy = 0.8924\n",
      "epoch 23, accuracy = 0.9003\n",
      "epoch 24, accuracy = 0.9017\n",
      "epoch 25, accuracy = 0.9042\n",
      "epoch 26, accuracy = 0.9014\n",
      "epoch 27, accuracy = 0.8991\n",
      "epoch 28, accuracy = 0.9076\n",
      "epoch 29, accuracy = 0.9012\n"
     ]
    }
   ],
   "source": [
    "# some tuning parameters\n",
    "N = length(trainy)\n",
    "m = 20       # batch size\n",
    "epochs = 50  # number of complete passes through the training data\n",
    "α = 0.01     # learning rate / step size\n",
    "λ = 0.01     # regularization parameter\n",
    "W, b, ∇W, ∇b = BGD(N, m, epochs, α=α, λ=λ)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
